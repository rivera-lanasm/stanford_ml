============= Local Regression
(unread) http://rafalab.github.io/pages/754/section-03.pdf


=========== Logistic regression
https://thelaziestprogrammer.com/sharrington/math-of-machine-learning/solving-logreg-newtons-method
https://medium.com/datadriveninvestor/machine-learning-sigmoid-function-softmax-function-and-exponential-family-2d0e4639d201
https://sebastianraschka.com/faq/docs/softmax_regression.html
http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/
http://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/



=========== Gradient descent 
https://thelaziestprogrammer.com/sharrington/math-of-machine-learning/gradient-descent-learning-rate-too-high
https://explained.ai/matrix-calculus/index.html


============ Kernel Methods 
https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf
https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c03_p47-84.pdf


============= regularization
https://medium.com/@zxr.nju/the-classical-linear-regression-model-is-good-why-do-we-need-regularization-c89dba10c8eb


=============== probabalistic graphical models
https://towardsdatascience.com/probabilistic-graphical-models-bayesian-networks-d8f0d51b14bf


=============== Max Entropy and information theory
https://sgfin.github.io/2017/03/16/Deriving-probability-distributions-using-the-Principle-of-Maximum-Entropy/
https://kconrad.math.uconn.edu/blurbs/analysis/entropypost.pdf
https://adityashrm21.github.io/Information-Theory-In-Deep-Learning/


=========== discriminant analysis
http://www.di.fc.ul.pt/~jpn/r/discriminant_analysis/discriminant_analysis.html
http://uc-r.github.io/discriminant_analysis




